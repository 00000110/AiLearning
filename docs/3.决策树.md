
# 第3章 决策树
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![决策树_首页](/images/3.DecisionTree/DecisionTree_headpage_xy.png "决策树首页")

## 决策树简介

![决策树-流程图](/images/3.DecisionTree/决策树-流程图.jpg "决策树示例流程图")

* 引入

    你是否玩过一个叫做 "二十个问题" 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。决策树的工作原理与 20 个问题类似，用户输入一系列数据，然后给出游戏的答案。 

* 简要介绍

    根据一些 feature 进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些数据是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上。

* 决策树的任务

    第二章的 k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义。决策树的主要优势就在于数据形式非常容易理解。
    
    接下来构造的决策树算法能够读取数据集合，构建类似于上图的决策树。决策树的一个重要任务是为了理解数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，这些机器根据数据集创建规则的过程，就是机器学习的过程。
    
    专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。

* 决策树的特点

    优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

    缺点：可能会产生过度匹配问题。
    
    适用数据类型：数值型和标称型。

## 在数据集中度量一致性

划分数据集的最大原则是: 将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。

在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。

学习了如何度量数据集的无序程度之后，分类算法除了需要测量信息熵，还需要划分数据集，度量划分数据集的熵，以便判断当前是否正确地划分了数据集。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。

## 使用递归构造决策树

> 构造决策树时需要解决的第一个问题

```
    在构造决策树时，我们需要解决的第一个问题就是，当前的数据集上哪个特征在划分数据分类时起决定性作用。
为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。
这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类，
无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。划分数据子集的算法和划分原始数据集的方法相同，
直到所有具有相同类型的数据均在一个数据子集内。
```

> 创建分支的伪代码函数createBranch()

```
    检测数据集中的每个子项是否属于同一分类：
        If so return 类标签
        Else
            寻找划分数据集的最好特征
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch 并增加返回结果到分支节点中
            return 分支节点
```

> 决策树的一般流程

```
    (1)收集数据：可以使用任何方法。
    (2)准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
    (3)分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
    (4)训练算法：构造树的数据结构。
    (5)测试算法：使用经验树计算错误率。
    (6)使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。
```

> 划分数据集时的数据路径

```
    得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。
    第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。
    递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，
    则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类，如下图所示：
```

![决策树划分数据集时的数据路径](/images/3.DecisionTree/决策树划分数据集时的数据路径.png)

## 使用Matplotlib绘制树形图

> Matplotlib绘制树形图示例

![Matplotlib绘制树形图示例](/images/3.DecisionTree/Matplotlib绘制树形图.png)

## 决策树小结

* 决策树是什么？
    * 顾名思义，是一种树，一种依托于策略抉择而建立起来的树。
    * 从数据产生决策树的机器学习技术叫做决策树学习, 通俗点说就是决策树。
* 决策树目前的情况：
    * 1.最经常使用的数据挖掘算法。(流行的原因：不需要了解机器学习的知识，就能搞明白决策树是如何工作的)
    * 2.数据形式【决策过程只有：是/否】和数据内在含义非常容易理解。
    * 3.决策树给出的结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。
* 决策树的构造：
    * 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
    * 缺点：可能会产生过度匹配问题。
    * 适用数据类型：数值型和标称型【标称型:其实就是离散型数据，变量的结果只在`有限`目标集中取值(例如：分类特征 A/B/C 类其中一种)】。
* 如何找出第一个分支点呢？
    * 信息增益： 
        * 划分数据集的最大原则是：将无序的数据变得更加有序。
        * 集合信息的度量称为`香农熵`或者简称`熵`(名字来源于信息论之父`克劳德·香农`)
        * 公式： 
            * \\(p(x_i)\\) 表示该 label 分类的概率
            * \\(l(x_i) = - \log_2p(x_i)\\) 表示符号\\(x_i\\)的信息定义
            * \\(H = -\sum_{i=0}^np(x_i)\log_2p(x_i)\\) 表示香农熵，用于计算信息熵
    * 基尼不纯度(Gini impurity)  [本书不做过多的介绍]
        * 简单来说：就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。
* 流程介绍图

![决策树流程介绍图](/images/3.DecisionTree/决策树流程介绍图.jpg)

```
    决策树分类器就像带有终止块的流程图，终止块表示分类结果。
    开始处理数据集时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。
    ID3算法可以用于划分标称型数据集。构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用
    Python 中内嵌的数据结构字典存储树节点信息。
```

* * *

* **作者：[片刻](http://www.apache.wiki/display/~jiangzhonglian) [小瑶](http://www.apache.wiki/display/~chenyao)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**        
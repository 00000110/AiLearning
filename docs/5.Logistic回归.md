# 第5章 Logistic回归
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

![朴素贝叶斯_首页](/images/5.Logistic/LogisticRegression_headPage_xy.png "Logistic回归首页")

## Sigmoid函数和Logistic回归分类器

> 回归简介

```
    假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。
    利用 Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，
    表示要找到最佳拟合参数集，其背后的数学分析将在下一部分介绍。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。
```

> Logistic回归特点

```
    优点：计算代价不高，易于理解和实现。
    缺点：容易欠拟合，分类精度可能不高。
    适用数据类型：数值型和标称型数据。
```

> Sigmoid函数简介

```
    Sigmoid函数具体的计算公式如下：
    f(z) = 1 / (1 + e ^(-z))
    图5-1 给出了Sigmoid函数在不同坐标尺度下的两条曲线图。当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近1；而随着x的减小，Sigmoid值将逼近0。
    如果横坐标刻度足够大（图5-1下图），Sigmoid函数看起来很像一个阶跃函数。
    因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间的数值。
    任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以，Logistic回归也可以被看成是一种概率估计。
```
![Logistic回归Sigmoid函数](/images/5.Logistic/Logistic回归Sigmoid函数.png "Logistic回归Sigmoid函数")

## 最优化理论初步

> 梯度上升法

```
    梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为▽，则函数f(x,y)的梯度由下式表示：
```

![logistic回归梯度上升法](/images/5.Logistic/梯度上升算法.png "梯度上升法")

梯度上升法的伪代码如下：
    某个回归系数初始化为1
    重复R次：
        计算整个数据集的梯度
        使用 alpha X grandient 更新回归系数的向量
    返回回归系数

梯度上升算法在每次回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，
那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。
由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，
一次处理所有数据被称作是“批处理”。
    随机梯度上升算法可以写成如下的伪代码：

    所有回归系数初始化为1
    对数据集中每个样本
        计算该样本的梯度
        使用 alpha X gradient 更新回归系数值
    返回回归系数值

## 梯度下降最优化算法

你最经常听到的应该是梯度下降算法，它与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成：
    w:=w-a▽f(w)
梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。

## 数据中的缺失项处理

数据中的缺失值是个非常棘手的问题，有很多文献都致力于解决这个问题。这个问题没有标准答案，取决于实际应用中的需求。
那么，数据缺失究竟带来了什么问题？假设有100个样本和20个特征，
这些数据都是机器手机回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？
它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。
* 下面给出了一些可选的做法：
    * 使用可用特征的均值来填补缺失值；
    * 使用特殊值来填补缺失值，如 -1；
    * 忽略有缺失值的样本；
    * 使用有相似样本的均值添补缺失值；
    * 使用另外的机器学习算法预测缺失值。

## Logistic 回归总结

* 逻辑回归(Logistic Regression)
    * 5.1 分类问题
        * 在分类问题中，尝试预测的是结果是否属于某一个类（例如正确或错误）。
        * 分类问题的例子有：
            * 判断一封电子邮件是否是垃圾邮件；
            * 判断一次金融交易是否是欺诈等等。
        * 从二元的分类问题开始讨论:
             将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量
             y属于{0，1}
             注：其中 0 表示负向类，1 表示正向类。
    * 5.2 假说表示

    * 5.3 判定边界
        * 在逻辑回归中，我们预测：
        
             当 hθ 大于等于 0.5 时，预测 y=1
             当 hθ 小于 0.5 时，预测 y=0
        * 根据上面绘制出的 S 形函数图像，我们知道当
             z=0时 ，g(z)=0.5
             z>0时 ，g(z)>0.5
             z<0时 ，g(z)<0.5
             又z=θ的T次方与X的积，即：
               z大于等于0时，预测：y=1
               z小于0时，预测：y=0
        * 现在假设我们有一个模型：Hθ(x)=g(θ0+θ1*x1+θ2*x2)
             并且参数θ是向量[-3 1 1]。则当-3+x1+x2大于等于0，即x1+x2大于等于3时，模型将预测y=1。
             我们可以绘制直线x1+x2=3，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。
        * 假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？
          因为需要用曲线才能分隔 y=0 的区域和 y=1 的区域，我们需要二次方特征： 假设参数是Hθ(x)=g(θ0+θ1*x1+θ2*x2+θ3*(x1^2)+θ4*(x2^2)+θ4*(x2^2))
          是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为 1 的圆形。可以用非常复杂的模型来适应非常复杂形状的判定边界。
    * 5.4 代价函数
    * 5.5 简化的成本函数和梯度下降
    * 5.6 高级优化
* * *

* **作者：[羊三](http://www.apache.wiki/display/~xuxin) [小瑶](http://www.apache.wiki/users/viewmyprofile.action)**
* [GitHub地址](https://github.com/apachecn/MachineLearning): <https://github.com/apachecn/MachineLearning>
* **版权声明：欢迎转载学习 => 请标注信息来源于 [ApacheCN](http://www.apache.wiki)**
